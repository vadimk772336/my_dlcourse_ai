{"cells":[{"cell_type":"markdown","metadata":{"id":"wU8nUWLwwwFA"},"source":["# Задание 1.2 - Линейный классификатор (Linear classifier)\n","\n","В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n","Тот класс, у которого эта сумма больше, и является предсказанием модели.\n","\n","В этом задании вы:\n","- потренируетесь считать градиенты различных многомерных функций\n","- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n","- реализуете процесс тренировки линейного классификатора\n","- подберете параметры тренировки на практике\n","\n","На всякий случай, еще раз ссылка на туториал по numpy:  \n","http://cs231n.github.io/python-numpy-tutorial/"]},{"cell_type":"code","source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd '/content/drive/MyDrive/Colab Notebooks/DL/dlcourse_ai/assignments/assignment1/'\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkgrDcyvxEWk","executionInfo":{"status":"ok","timestamp":1646828327879,"user_tz":-420,"elapsed":2320,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"5d6cfba6-244c-4e0b-ac02-175cbd0979ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/DL/dlcourse_ai/assignments/assignment1\n"]}]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"iNIfpnyj9lEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eQ6fLo7cwwFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828327883,"user_tz":-420,"elapsed":16,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"3b7579c7-d059-4a45-f4b1-92bcdbd8a3f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxJUt5wEwwFK"},"outputs":[],"source":["from dataset import load_svhn, random_split_train_val\n","from gradient_check import check_gradient\n","from metrics import multiclass_accuracy \n","import linear_classifer"]},{"cell_type":"markdown","metadata":{"id":"gwXoC3eTwwFL"},"source":["# Как всегда, первым делом загружаем данные\n","\n","Мы будем использовать все тот же SVHN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QK10m1twwFM"},"outputs":[],"source":["def prepare_for_linear_classifier(train_X, test_X):\n","    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n","    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n","    \n","    # Subtract mean\n","    mean_image = np.mean(train_flat, axis = 0)\n","    train_flat -= mean_image\n","    test_flat -= mean_image\n","    \n","    # Add another channel with ones as a bias term\n","    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n","    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n","    return train_flat_with_ones, test_flat_with_ones\n","    \n","train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n","train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n","# Split train into train and val\n","train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"]},{"cell_type":"markdown","metadata":{"id":"aQPeOhMTwwFN"},"source":["# Играемся с градиентами!\n","\n","В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n","\n","Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n","Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n","```\n","def f(x):\n","    \"\"\"\n","    Computes function and analytic gradient at x\n","    \n","    x: np array of float, input to the function\n","    \n","    Returns:\n","    value: float, value of the function \n","    grad: np array of float, same shape as x\n","    \"\"\"\n","    ...\n","    \n","    return value, grad\n","```\n","\n","Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n","\n","Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n","\n","Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n","\n","![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n","\n","Все функции приведенные в следующей клетке должны проходить gradient check."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"zYzDhYK8wwFP","executionInfo":{"status":"ok","timestamp":1646828332811,"user_tz":-420,"elapsed":633,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"f523cc27-9b40-452e-f9fb-2b731a5b65ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n","Gradient check passed!\n","Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}],"source":["# TODO: Implement check_gradient function in gradient_check.py\n","# All the functions below should pass the gradient check\n","\n","def square(x):\n","    return float(x*x), 2*x\n","\n","check_gradient(square, np.array([3.0]))\n","\n","def array_sum(x):\n","    assert x.shape == (2,), x.shape\n","    return np.sum(x), np.ones_like(x)\n","\n","check_gradient(array_sum, np.array([3.0, 2.0]))\n","\n","def array_2d_sum(x):\n","    assert x.shape == (2,2)\n","    return np.sum(x), np.ones_like(x)\n","\n","check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"]},{"cell_type":"markdown","metadata":{"id":"E-ay2i8hwwFR"},"source":["## Начинаем писать свои функции, считающие аналитический градиент\n","\n","Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n","![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n","\n","**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n","\n","К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n","```\n","predictions -= np.max(predictions)\n","```\n","(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Qu7UEsJwwFS"},"outputs":[],"source":["# TODO Implement softmax and cross-entropy for single sample\n","probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n","\n","# Make sure it works for big numbers too!\n","probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n","assert np.isclose(probs[0], 1.0)"]},{"cell_type":"markdown","metadata":{"id":"cqmpyPN_wwFT"},"source":["Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n","В общем виде cross-entropy определена следующим образом:\n","![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n","\n","где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n","В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n","\n","Это позволяет реализовать функцию проще!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPNgsstowwFU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828332812,"user_tz":-420,"elapsed":35,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"b122fabf-3f22-42d3-e496-7709e56c45ae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.006760443547122"]},"metadata":{},"execution_count":17}],"source":["probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n","linear_classifer.cross_entropy_loss(probs, 1)"]},{"cell_type":"markdown","metadata":{"id":"DXuveZCrwwFU"},"source":["После того как мы реализовали сами функции, мы можем реализовать градиент.\n","\n","Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n","\n","Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uu4GOSQtwwFV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828332813,"user_tz":-420,"elapsed":28,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"a66c2a0b-adad-43c9-8bd5-90dbc805330d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}],"source":["# TODO Implement combined function or softmax and cross entropy and produces gradient\n","loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n","check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"]},{"cell_type":"markdown","metadata":{"id":"zEqtWw_8wwFW"},"source":["В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n","\n","Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n","\n","Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n","\n","Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"8k7IDXCbwwFW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828332814,"user_tz":-420,"elapsed":22,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"826b4e58-5f49-4cd1-ac07-f28d41c87113"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n","Gradient check passed!\n"]}],"source":["# TODO Extend combined function so it can receive a 2d array with batch of samples\n","np.random.seed(42)\n","\n","# Test batch_size = 1\n","num_classes = 4\n","batch_size = 1\n","predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n","target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n","check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n","\n","# Test batch_size = 3\n","num_classes = 4\n","batch_size = 3\n","predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n","target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n","check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n","\n","# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n","probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n","assert np.all(np.isclose(probs[:, 0], 1.0))"]},{"cell_type":"markdown","metadata":{"id":"FxZjeZuEwwFX"},"source":["### Наконец, реализуем сам линейный классификатор!\n","\n","softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n","\n","Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n","\n","Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n","\n","`predictions = X * W`, где `*` - матричное умножение.\n","\n","Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zF8ApsXIwwFX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828332814,"user_tz":-420,"elapsed":18,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"df8ca602-09c9-4aa2-9c7e-cae73fa7762e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":20}],"source":["# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n","batch_size = 2\n","num_classes = 2\n","num_features = 3\n","np.random.seed(42)\n","W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n","X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n","target_index = np.ones(batch_size, dtype=np.int)\n","\n","loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n","check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"]},{"cell_type":"markdown","metadata":{"id":"ZkEgX6xCwwFY"},"source":["### И теперь регуляризация\n","\n","Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n","\n","Напомним, L2 regularization определяется как\n","\n","l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n","\n","Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TASDJrqYwwFY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828332815,"user_tz":-420,"elapsed":15,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"874173e5-6aac-42d2-8b6e-1b1f7bf01ad0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":21}],"source":["# TODO Implement l2_regularization function that implements loss for L2 regularization\n","linear_classifer.l2_regularization(W, 0.01)\n","check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"]},{"cell_type":"markdown","metadata":{"id":"_Ic2XBx9wwFZ"},"source":["# Тренировка!"]},{"cell_type":"markdown","metadata":{"id":"sfwaG2uFwwFZ"},"source":["Градиенты в порядке, реализуем процесс тренировки!"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"H9ueovmZwwFa"},"outputs":[],"source":["# TODO: Implement LinearSoftmaxClassifier.fit function\n","classifier = linear_classifer.LinearSoftmaxClassifier()\n","loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVQAGAiFwwFa","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1646828336682,"user_tz":-420,"elapsed":447,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"98d223f3-12c2-4025-ded4-8a640a6587a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f302af0aad0>]"]},"metadata":{},"execution_count":23},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["# let's look at the loss history!\n","plt.plot(loss_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yo1CLUeFwwFb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828370858,"user_tz":-420,"elapsed":34190,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"806a0fe9-a7af-4a35-d6cd-4cfc7bdf0471"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.127\n","Accuracy after training for 100 epochs:  0.121\n"]}],"source":["# Let's check how it performs on validation set\n","pred = classifier.predict(val_X)\n","accuracy = multiclass_accuracy(pred, val_y)\n","print(\"Accuracy: \", accuracy)\n","\n","# Now, let's train more and see if it performs better\n","classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n","pred = classifier.predict(val_X)\n","accuracy = multiclass_accuracy(pred, val_y)\n","print(\"Accuracy after training for 100 epochs: \", accuracy)"]},{"cell_type":"markdown","metadata":{"id":"o2fSlDXxwwFb"},"source":["### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n","\n","В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n","\n","Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n","Добейтесь точности более чем **20%** на проверочных данных (validation data)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-W_LCFk3wwFb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a4d5e60-f8be-4720-9da2-df6fc23a785d","executionInfo":{"status":"ok","timestamp":1646828971338,"user_tz":-420,"elapsed":600492,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0.001 1e-06 0.23\n","0.001 5.5e-06 0.23\n","0.001 1e-05 0.23\n","0.0055000000000000005 1e-06 0.242\n","0.0055000000000000005 5.5e-06 0.243\n","0.0055000000000000005 1e-05 0.244\n","0.01 1e-06 0.244\n","0.01 5.5e-06 0.244\n","0.01 1e-05 0.244\n","best hyperparameters: learning_rate: 6e-03, reg_strength: 1e-05\n","best validation accuracy achieved: 0.244000\n"]}],"source":["num_epochs = 200\n","batch_size = 300\n","\n","#learning_rates = [1e-3, 1e-4, 1e-5]\n","#reg_strengths = [1e-4, 1e-5, 1e-6]\n","\n","learning_rates = np.linspace(1e-3,1e-2,3)\n","reg_strengths = np.linspace(1e-6,1e-5,3)\n","\n","best_classifier = None\n","best_val_accuracy = None\n","\n","# TODO use validation set to find the best hyperparameters\n","# hint: for best results, you might need to try more values for learning rate and regularization strength \n","# than provided initially\n","\n","best_val_accuracy = -1\n","best_params = (-1,-1)\n","\n","for learning_rate in learning_rates:\n","    for reg_strength in reg_strengths:\n","      classifier = linear_classifer.LinearSoftmaxClassifier()\n","      classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=learning_rate, batch_size = batch_size, reg=reg_strength)\n","      pred = classifier.predict(val_X)\n","      accuracy = multiclass_accuracy(pred, val_y)\n","      if best_val_accuracy < accuracy:\n","        best_params = (learning_rate,reg_strength)\n","        best_val_accuracy = accuracy\n","        best_classifier = classifier\n","      print(learning_rate,reg_strength,best_val_accuracy)\n","\n","print(\"best hyperparameters: learning_rate: %1.0e, reg_strength: %1.0e\" % (best_params))\n","print('best validation accuracy achieved: %f' % best_val_accuracy)"]},{"cell_type":"code","source":[""],"metadata":{"id":"-zuzWRlnYGkT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w1gJe88xwwFc"},"source":["# Какой же точности мы добились на тестовых данных?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCv7p7GOwwFc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646828971339,"user_tz":-420,"elapsed":13,"user":{"displayName":"Вадим К","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrVNY2eGGaQbUC8BkoSLTabkdfXee8bDXcNiyo=s64","userId":"15055313511788181293"}},"outputId":"0ea1b38d-8db3-45fd-ae00-948cfd1472f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Linear softmax classifier test set accuracy: 0.203000\n"]}],"source":["test_pred = best_classifier.predict(test_X)\n","test_accuracy = multiclass_accuracy(test_pred, test_y)\n","print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Linear classifier.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}