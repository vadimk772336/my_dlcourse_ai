{"cells":[{"cell_type":"markdown","metadata":{"id":"oPgaJ11grucP"},"source":["# Задание 6: Рекуррентные нейронные сети (RNNs)\n","\n","Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd '/content/drive/MyDrive/Colab Notebooks/DL/dlcourse_ai/assignments/assignment6/'\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bf2FxH5fsyrt","executionInfo":{"status":"ok","timestamp":1653143928963,"user_tz":-420,"elapsed":2558,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"44b51b0b-9692-4b1d-df32-572fd0c2d253"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/DL/dlcourse_ai/assignments/assignment6\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"P59NYU98GCb9","executionInfo":{"status":"ok","timestamp":1653143928968,"user_tz":-420,"elapsed":31,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"1e4957bb-4918-403c-9d30-027250f31a65"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n!pip3 -qq install torch==0.4.1\\n!pip3 -qq install bokeh==0.13.0\\n!pip3 -qq install gensim==3.6.0\\n!pip3 -qq install nltk\\n!pip3 -qq install scikit-learn==0.20.2\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["'''\n","!pip3 -qq install torch==0.4.1\n","!pip3 -qq install bokeh==0.13.0\n","!pip3 -qq install gensim==3.6.0\n","!pip3 -qq install nltk\n","!pip3 -qq install scikit-learn==0.20.2\n","'''"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8sVtGHmA9aBM","executionInfo":{"status":"ok","timestamp":1653143929978,"user_tz":-420,"elapsed":1033,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"outputs":[],"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor, LongTensor\n","else:\n","    from torch import FloatTensor, LongTensor\n","\n","np.random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"-6CNKM3b4hT1"},"source":["# Рекуррентные нейронные сети (RNNs)"]},{"cell_type":"markdown","metadata":{"id":"O_XkoGNQUeGm"},"source":["## POS Tagging"]},{"cell_type":"markdown","metadata":{"id":"QFEtWrS_4rUs"},"source":["Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n","\n","![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n","\n","*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n","\n","Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n","\n","Мы порешаем сейчас POS Tagging для английского.\n","\n","Будем работать с таким набором тегов:\n","- ADJ - adjective (new, good, high, ...)\n","- ADP - adposition (on, of, at, ...)\n","- ADV - adverb (really, already, still, ...)\n","- CONJ - conjunction (and, or, but, ...)\n","- DET - determiner, article (the, a, some, ...)\n","- NOUN - noun (year, home, costs, ...)\n","- NUM - numeral (twenty-four, fourth, 1991, ...)\n","- PRT - particle (at, on, out, ...)\n","- PRON - pronoun (he, their, her, ...)\n","- VERB - verb (is, say, told, ...)\n","- . - punctuation marks (. , ;)\n","- X - other (ersatz, esprit, dunno, ...)"]},{"cell_type":"markdown","metadata":{"id":"EPIkKdFlHB-X"},"source":["Скачаем данные:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiA2dGmgF1rW","executionInfo":{"status":"ok","timestamp":1653143931969,"user_tz":-420,"elapsed":1997,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"b5c4801b-035f-4839-ed02-bc0106141ed2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  dtype=np.int):\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, positive=False):\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  precompute=False, eps=np.finfo(np.float).eps,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=np.finfo(np.float).eps, random_state=None,\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"]},{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]}],"source":["import nltk\n","from sklearn.model_selection import train_test_split\n","\n","nltk.download('brown')\n","nltk.download('universal_tagset')\n","\n","data = nltk.corpus.brown.tagged_sents(tagset='universal')"]},{"cell_type":"markdown","metadata":{"id":"d93g_swyJA_V"},"source":["Пример размеченного предложения:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QstS4NO0L97c","executionInfo":{"status":"ok","timestamp":1653143931971,"user_tz":-420,"elapsed":20,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"57b189b0-2848-4a23-e03b-e7a90cec6e2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["The            \tDET\n","Fulton         \tNOUN\n","County         \tNOUN\n","Grand          \tADJ\n","Jury           \tNOUN\n","said           \tVERB\n","Friday         \tNOUN\n","an             \tDET\n","investigation  \tNOUN\n","of             \tADP\n","Atlanta's      \tNOUN\n","recent         \tADJ\n","primary        \tNOUN\n","election       \tNOUN\n","produced       \tVERB\n","``             \t.\n","no             \tDET\n","evidence       \tNOUN\n","''             \t.\n","that           \tADP\n","any            \tDET\n","irregularities \tNOUN\n","took           \tVERB\n","place          \tNOUN\n",".              \t.\n"]}],"source":["for word, tag in data[0]:\n","    print('{:15}\\t{}'.format(word, tag))"]},{"cell_type":"markdown","metadata":{"id":"epdW8u_YXcAv"},"source":["Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n","\n","На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xTai8Ta0lgwL","executionInfo":{"status":"ok","timestamp":1653143970992,"user_tz":-420,"elapsed":39034,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"fd907df9-2bf7-43ee-ae3c-98f5c94a49a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Words count in train set: 739769\n","Words count in val set: 130954\n","Words count in test set: 290469\n"]}],"source":["train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n","train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n","\n","print('Words count in train set:', sum(len(sent) for sent in train_data))\n","print('Words count in val set:', sum(len(sent) for sent in val_data))\n","print('Words count in test set:', sum(len(sent) for sent in test_data))"]},{"cell_type":"markdown","metadata":{"id":"eChdLNGtXyP0"},"source":["Построим маппинги из слов в индекс и из тега в индекс:\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pCjwwDs6Zq9x","executionInfo":{"status":"ok","timestamp":1653143971374,"user_tz":-420,"elapsed":419,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"0de99a8d-ee58-4631-a52f-a43c7d474609"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique words in train = 45441. Tags = {'NOUN', 'VERB', '.', 'CONJ', 'X', 'ADP', 'NUM', 'PRT', 'ADJ', 'PRON', 'ADV', 'DET'}\n"]}],"source":["words = {word for sample in train_data for word, tag in sample}\n","word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n","word2ind['<pad>'] = 0\n","\n","tags = {tag for sample in train_data for word, tag in sample}\n","tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n","tag2ind['<pad>'] = 0\n","\n","print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"URC1B2nvPGFt","executionInfo":{"status":"ok","timestamp":1653143971772,"user_tz":-420,"elapsed":402,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"d1753c8d-eda1-4c2f-c94a-fd4eb3ed4f75"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from collections import Counter\n","\n","tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n","tag_distribution = [tag_distribution[tag] for tag in tags]\n","\n","plt.figure(figsize=(10, 5))\n","\n","bar_width = 0.35\n","plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n","plt.xticks(np.arange(len(tags)), tags)\n","    \n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gArQwbzWWkgi"},"source":["## Бейзлайн\n","\n","Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n","\n","![tag-context](https://www.nltk.org/images/tag-context.png)  \n","*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n","\n","На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n","\n","Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n","\n","Простейший вариант - униграммная модель, учитывающая только слово:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rWmSToIaeAo","executionInfo":{"status":"ok","timestamp":1653143975973,"user_tz":-420,"elapsed":4211,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"0483eba0-eff9-4ede-ffe1-c326d209f9a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of unigram tagger = 92.62%\n"]}],"source":["import nltk\n","\n","default_tagger = nltk.DefaultTagger('NN')\n","\n","unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n","print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"]},{"cell_type":"markdown","metadata":{"id":"07Ymb_MkbWsF"},"source":["Добавим вероятности переходов:"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vjz_Rk0bbMyH","executionInfo":{"status":"ok","timestamp":1653143981958,"user_tz":-420,"elapsed":6016,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"5e9df75d-789b-44e7-d42d-b0ef28715731"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of bigram tagger = 93.42%\n"]}],"source":["bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n","print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"]},{"cell_type":"markdown","metadata":{"id":"uWMw6QHvbaDd"},"source":["Обратите внимание, что `backoff` важен:"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8XCuxEBVbOY_","executionInfo":{"status":"ok","timestamp":1653143986873,"user_tz":-420,"elapsed":4936,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"fb44bf51-a8de-42e6-fc2e-b8dba48c9124"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of trigram tagger = 23.33%\n"]}],"source":["trigram_tagger = nltk.TrigramTagger(train_data)\n","print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"]},{"cell_type":"markdown","metadata":{"id":"4t3xyYd__8d-"},"source":["## Увеличиваем контекст с рекуррентными сетями\n","\n","Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n","\n","Омонимия - основная причина, почему униграмная модель плоха:  \n","*“he cashed a check at the **bank**”*  \n","vs  \n","*“he sat on the **bank** of the river”*\n","\n","Поэтому нам очень полезно учитывать контекст при предсказании тега.\n","\n","Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n","\n","![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n","\n","Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"RtRbz1SwgEqc","executionInfo":{"status":"ok","timestamp":1653143988196,"user_tz":-420,"elapsed":1335,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"outputs":[],"source":["def convert_data(data, word2ind, tag2ind):\n","    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n","    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n","    \n","    return X, y\n","\n","X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n","X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n","X_test, y_test = convert_data(test_data, word2ind, tag2ind)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"DhsTKZalfih6","executionInfo":{"status":"ok","timestamp":1653143988197,"user_tz":-420,"elapsed":23,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"outputs":[],"source":["def iterate_batches(data, batch_size):\n","    X, y = data\n","    n_samples = len(X)\n","\n","    indices = np.arange(n_samples)\n","    np.random.shuffle(indices)\n","    \n","    for start in range(0, n_samples, batch_size):\n","        end = min(start + batch_size, n_samples)\n","        \n","        batch_indices = indices[start:end]\n","        \n","        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n","        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n","        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n","        \n","        for batch_ind, sample_ind in enumerate(batch_indices):\n","            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n","            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n","            \n","        yield X_batch, y_batch"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l4XsRII5kW5x","executionInfo":{"status":"ok","timestamp":1653143988199,"user_tz":-420,"elapsed":23,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"d316392a-140e-4f40-eba1-8aa6897c6fb3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((32, 4), (32, 4))"]},"metadata":{},"execution_count":14}],"source":["X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n","\n","X_batch.shape, y_batch.shape"]},{"cell_type":"markdown","metadata":{"id":"C5I9E9P6eFYv"},"source":["**Задание** Реализуйте `LSTMTagger`:"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"WVEHju54d68T","executionInfo":{"status":"ok","timestamp":1653143988200,"user_tz":-420,"elapsed":17,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"outputs":[],"source":["class LSTMTagger(nn.Module):\n","    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n","        super().__init__()\n","        \n","        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n","        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n","        self._out_layer = nn.Linear(lstm_hidden_dim, tagset_size)\n","\n","    def forward(self, inputs):\n","\n","        emb = self._emb(inputs)\n","        output = self._lstm(emb)[0]\n","        out = self._out_layer(output)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"q_HA8zyheYGH"},"source":["**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jbrxsZ2mehWB","executionInfo":{"status":"ok","timestamp":1653143988200,"user_tz":-420,"elapsed":17,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"b96a799c-9fe0-44c7-a505-e2408a428e0f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.05434782608695652"]},"metadata":{},"execution_count":16}],"source":["model = LSTMTagger(\n","    vocab_size=len(word2ind),\n","    tagset_size=len(tag2ind)\n",")\n","\n","X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n","\n","logits = model(X_batch)\n","\n","pred = torch.argmax(logits, dim = 2)\n","mask = (y_batch != 0)\n","correct_count = ((pred == y_batch) * mask).sum().item()\n","total_count = mask.sum().item()\n","correct_count / total_count"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GMUyUm1hgpe3","executionInfo":{"status":"ok","timestamp":1653143988201,"user_tz":-420,"elapsed":14,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"0bf07567-7045-42db-b5b9-d9a12aa157b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(82.4854, grad_fn=<AddBackward0>)\n"]}],"source":["criterion = nn.CrossEntropyLoss(ignore_index = 0)\n","\n","loss = 0\n","for ind, row in enumerate(logits):\n","    loss += criterion(row, y_batch[ind])\n","print(loss)"]},{"cell_type":"markdown","metadata":{"id":"nSgV3NPUpcjH"},"source":["**Задание** Вставьте эти вычисление в функцию:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"FprPQ0gllo7b","executionInfo":{"status":"ok","timestamp":1653143988201,"user_tz":-420,"elapsed":11,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"outputs":[],"source":["import math\n","from tqdm import tqdm\n","\n","\n","def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n","    epoch_loss = 0\n","    correct_count = 0\n","    sum_count = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = math.ceil(len(data[0]) / batch_size)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n","                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n","                logits = model(X_batch)\n","\n","                loss = 0\n","                for ind, row in enumerate(logits):\n","                    loss += criterion(row, y_batch[ind])\n","\n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","\n","\n","                pred = torch.argmax(logits, dim = 2)\n","                mask = (y_batch != 0)\n","                cur_correct_count, cur_sum_count = ((pred == y_batch) * mask).sum().item(), mask.sum().item()\n","\n","                correct_count += cur_correct_count\n","                sum_count += cur_sum_count\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","                    name, loss.item(), cur_correct_count / cur_sum_count)\n","                )\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","                name, epoch_loss / batches_count, correct_count / sum_count)\n","            )\n","\n","    return epoch_loss / batches_count, correct_count / sum_count\n","\n","\n","def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n","        val_data=None, val_batch_size=None):\n","        \n","    if not val_data is None and val_batch_size is None:\n","        val_batch_size = batch_size\n","        \n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_data is None:\n","            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pqfbeh1ltEYa","executionInfo":{"status":"ok","timestamp":1653145010650,"user_tz":-420,"elapsed":1022459,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"872e3e0a-f45b-485f-d2b0-8227797b82d1"},"outputs":[{"output_type":"stream","name":"stderr","text":["[1 / 50] Train: Loss = 49.56588, Accuracy = 73.83%: 100%|██████████| 572/572 [00:19<00:00, 28.94it/s]\n","[1 / 50]   Val: Loss = 46.46846, Accuracy = 83.01%: 100%|██████████| 13/13 [00:00<00:00, 35.18it/s]\n","[2 / 50] Train: Loss = 22.94228, Accuracy = 87.42%: 100%|██████████| 572/572 [00:20<00:00, 28.11it/s]\n","[2 / 50]   Val: Loss = 32.98179, Accuracy = 88.29%: 100%|██████████| 13/13 [00:00<00:00, 33.21it/s]\n","[3 / 50] Train: Loss = 15.79891, Accuracy = 91.31%: 100%|██████████| 572/572 [00:19<00:00, 29.16it/s]\n","[3 / 50]   Val: Loss = 26.47064, Accuracy = 90.74%: 100%|██████████| 13/13 [00:00<00:00, 33.74it/s]\n","[4 / 50] Train: Loss = 11.74396, Accuracy = 93.37%: 100%|██████████| 572/572 [00:19<00:00, 29.37it/s]\n","[4 / 50]   Val: Loss = 26.08363, Accuracy = 91.77%: 100%|██████████| 13/13 [00:00<00:00, 35.31it/s]\n","[5 / 50] Train: Loss = 8.74903, Accuracy = 94.66%: 100%|██████████| 572/572 [00:19<00:00, 29.09it/s]\n","[5 / 50]   Val: Loss = 22.41928, Accuracy = 92.71%: 100%|██████████| 13/13 [00:00<00:00, 34.28it/s]\n","[6 / 50] Train: Loss = 6.80665, Accuracy = 95.54%: 100%|██████████| 572/572 [00:19<00:00, 28.93it/s]\n","[6 / 50]   Val: Loss = 23.59162, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 33.97it/s]\n","[7 / 50] Train: Loss = 5.33986, Accuracy = 96.24%: 100%|██████████| 572/572 [00:19<00:00, 28.65it/s]\n","[7 / 50]   Val: Loss = 21.24729, Accuracy = 93.61%: 100%|██████████| 13/13 [00:00<00:00, 33.82it/s]\n","[8 / 50] Train: Loss = 4.27285, Accuracy = 96.79%: 100%|██████████| 572/572 [00:19<00:00, 28.71it/s]\n","[8 / 50]   Val: Loss = 23.25954, Accuracy = 93.72%: 100%|██████████| 13/13 [00:00<00:00, 32.87it/s]\n","[9 / 50] Train: Loss = 3.52947, Accuracy = 97.21%: 100%|██████████| 572/572 [00:20<00:00, 28.32it/s]\n","[9 / 50]   Val: Loss = 23.00862, Accuracy = 93.89%: 100%|██████████| 13/13 [00:00<00:00, 34.73it/s]\n","[10 / 50] Train: Loss = 2.87352, Accuracy = 97.57%: 100%|██████████| 572/572 [00:19<00:00, 28.86it/s]\n","[10 / 50]   Val: Loss = 24.50207, Accuracy = 94.03%: 100%|██████████| 13/13 [00:00<00:00, 35.59it/s]\n","[11 / 50] Train: Loss = 2.37402, Accuracy = 97.93%: 100%|██████████| 572/572 [00:20<00:00, 28.35it/s]\n","[11 / 50]   Val: Loss = 27.66714, Accuracy = 94.08%: 100%|██████████| 13/13 [00:00<00:00, 32.77it/s]\n","[12 / 50] Train: Loss = 1.95987, Accuracy = 98.19%: 100%|██████████| 572/572 [00:19<00:00, 28.79it/s]\n","[12 / 50]   Val: Loss = 27.57079, Accuracy = 94.17%: 100%|██████████| 13/13 [00:00<00:00, 33.48it/s]\n","[13 / 50] Train: Loss = 1.63306, Accuracy = 98.45%: 100%|██████████| 572/572 [00:19<00:00, 28.63it/s]\n","[13 / 50]   Val: Loss = 32.31480, Accuracy = 94.15%: 100%|██████████| 13/13 [00:00<00:00, 32.54it/s]\n","[14 / 50] Train: Loss = 1.33967, Accuracy = 98.68%: 100%|██████████| 572/572 [00:19<00:00, 28.64it/s]\n","[14 / 50]   Val: Loss = 31.25406, Accuracy = 94.15%: 100%|██████████| 13/13 [00:00<00:00, 32.17it/s]\n","[15 / 50] Train: Loss = 1.11258, Accuracy = 98.88%: 100%|██████████| 572/572 [00:19<00:00, 29.14it/s]\n","[15 / 50]   Val: Loss = 32.32903, Accuracy = 94.14%: 100%|██████████| 13/13 [00:00<00:00, 35.00it/s]\n","[16 / 50] Train: Loss = 0.88103, Accuracy = 99.08%: 100%|██████████| 572/572 [00:19<00:00, 29.17it/s]\n","[16 / 50]   Val: Loss = 37.31423, Accuracy = 94.10%: 100%|██████████| 13/13 [00:00<00:00, 33.19it/s]\n","[17 / 50] Train: Loss = 0.72931, Accuracy = 99.23%: 100%|██████████| 572/572 [00:19<00:00, 29.35it/s]\n","[17 / 50]   Val: Loss = 37.90624, Accuracy = 94.08%: 100%|██████████| 13/13 [00:00<00:00, 33.47it/s]\n","[18 / 50] Train: Loss = 0.60225, Accuracy = 99.36%: 100%|██████████| 572/572 [00:19<00:00, 29.20it/s]\n","[18 / 50]   Val: Loss = 37.35900, Accuracy = 94.03%: 100%|██████████| 13/13 [00:00<00:00, 34.64it/s]\n","[19 / 50] Train: Loss = 0.47314, Accuracy = 99.48%: 100%|██████████| 572/572 [00:19<00:00, 29.10it/s]\n","[19 / 50]   Val: Loss = 40.78899, Accuracy = 94.12%: 100%|██████████| 13/13 [00:00<00:00, 34.18it/s]\n","[20 / 50] Train: Loss = 0.37165, Accuracy = 99.58%: 100%|██████████| 572/572 [00:19<00:00, 29.13it/s]\n","[20 / 50]   Val: Loss = 42.21748, Accuracy = 94.05%: 100%|██████████| 13/13 [00:00<00:00, 33.60it/s]\n","[21 / 50] Train: Loss = 0.30962, Accuracy = 99.64%: 100%|██████████| 572/572 [00:19<00:00, 28.72it/s]\n","[21 / 50]   Val: Loss = 46.11314, Accuracy = 94.01%: 100%|██████████| 13/13 [00:00<00:00, 32.58it/s]\n","[22 / 50] Train: Loss = 0.25287, Accuracy = 99.70%: 100%|██████████| 572/572 [00:20<00:00, 28.44it/s]\n","[22 / 50]   Val: Loss = 48.62775, Accuracy = 94.03%: 100%|██████████| 13/13 [00:00<00:00, 34.17it/s]\n","[23 / 50] Train: Loss = 0.31166, Accuracy = 99.65%: 100%|██████████| 572/572 [00:20<00:00, 28.60it/s]\n","[23 / 50]   Val: Loss = 44.89113, Accuracy = 93.98%: 100%|██████████| 13/13 [00:00<00:00, 33.37it/s]\n","[24 / 50] Train: Loss = 0.26192, Accuracy = 99.69%: 100%|██████████| 572/572 [00:19<00:00, 28.84it/s]\n","[24 / 50]   Val: Loss = 43.75602, Accuracy = 94.04%: 100%|██████████| 13/13 [00:00<00:00, 34.59it/s]\n","[25 / 50] Train: Loss = 0.17040, Accuracy = 99.78%: 100%|██████████| 572/572 [00:20<00:00, 27.86it/s]\n","[25 / 50]   Val: Loss = 54.68144, Accuracy = 94.05%: 100%|██████████| 13/13 [00:00<00:00, 33.46it/s]\n","[26 / 50] Train: Loss = 0.13906, Accuracy = 99.81%: 100%|██████████| 572/572 [00:19<00:00, 28.82it/s]\n","[26 / 50]   Val: Loss = 54.56662, Accuracy = 94.02%: 100%|██████████| 13/13 [00:00<00:00, 33.45it/s]\n","[27 / 50] Train: Loss = 0.13484, Accuracy = 99.82%: 100%|██████████| 572/572 [00:19<00:00, 28.75it/s]\n","[27 / 50]   Val: Loss = 52.78607, Accuracy = 93.92%: 100%|██████████| 13/13 [00:00<00:00, 34.24it/s]\n","[28 / 50] Train: Loss = 0.17098, Accuracy = 99.78%: 100%|██████████| 572/572 [00:20<00:00, 27.72it/s]\n","[28 / 50]   Val: Loss = 57.07864, Accuracy = 93.90%: 100%|██████████| 13/13 [00:00<00:00, 32.27it/s]\n","[29 / 50] Train: Loss = 0.83649, Accuracy = 99.28%: 100%|██████████| 572/572 [00:19<00:00, 28.97it/s]\n","[29 / 50]   Val: Loss = 51.55972, Accuracy = 93.97%: 100%|██████████| 13/13 [00:00<00:00, 33.56it/s]\n","[30 / 50] Train: Loss = 0.23244, Accuracy = 99.72%: 100%|██████████| 572/572 [00:20<00:00, 28.55it/s]\n","[30 / 50]   Val: Loss = 52.48052, Accuracy = 94.02%: 100%|██████████| 13/13 [00:00<00:00, 34.05it/s]\n","[31 / 50] Train: Loss = 0.12853, Accuracy = 99.83%: 100%|██████████| 572/572 [00:20<00:00, 28.07it/s]\n","[31 / 50]   Val: Loss = 58.66733, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 33.85it/s]\n","[32 / 50] Train: Loss = 0.10214, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.54it/s]\n","[32 / 50]   Val: Loss = 59.56801, Accuracy = 94.10%: 100%|██████████| 13/13 [00:00<00:00, 33.41it/s]\n","[33 / 50] Train: Loss = 0.09409, Accuracy = 99.85%: 100%|██████████| 572/572 [00:20<00:00, 28.41it/s]\n","[33 / 50]   Val: Loss = 59.91284, Accuracy = 94.08%: 100%|██████████| 13/13 [00:00<00:00, 32.57it/s]\n","[34 / 50] Train: Loss = 0.09047, Accuracy = 99.85%: 100%|██████████| 572/572 [00:19<00:00, 28.71it/s]\n","[34 / 50]   Val: Loss = 53.85732, Accuracy = 94.05%: 100%|██████████| 13/13 [00:00<00:00, 33.83it/s]\n","[35 / 50] Train: Loss = 0.09035, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.83it/s]\n","[35 / 50]   Val: Loss = 63.27470, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 33.08it/s]\n","[36 / 50] Train: Loss = 0.09045, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.35it/s]\n","[36 / 50]   Val: Loss = 56.40821, Accuracy = 94.06%: 100%|██████████| 13/13 [00:00<00:00, 35.24it/s]\n","[37 / 50] Train: Loss = 0.09031, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.29it/s]\n","[37 / 50]   Val: Loss = 58.67880, Accuracy = 94.02%: 100%|██████████| 13/13 [00:00<00:00, 33.76it/s]\n","[38 / 50] Train: Loss = 0.09010, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.27it/s]\n","[38 / 50]   Val: Loss = 66.38891, Accuracy = 94.04%: 100%|██████████| 13/13 [00:00<00:00, 32.56it/s]\n","[39 / 50] Train: Loss = 0.24245, Accuracy = 99.72%: 100%|██████████| 572/572 [00:20<00:00, 27.89it/s]\n","[39 / 50]   Val: Loss = 40.83262, Accuracy = 93.93%: 100%|██████████| 13/13 [00:00<00:00, 35.12it/s]\n","[40 / 50] Train: Loss = 0.74849, Accuracy = 99.35%: 100%|██████████| 572/572 [00:19<00:00, 28.64it/s]\n","[40 / 50]   Val: Loss = 42.35743, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 33.97it/s]\n","[41 / 50] Train: Loss = 0.19028, Accuracy = 99.75%: 100%|██████████| 572/572 [00:20<00:00, 28.46it/s]\n","[41 / 50]   Val: Loss = 49.25069, Accuracy = 94.12%: 100%|██████████| 13/13 [00:00<00:00, 33.79it/s]\n","[42 / 50] Train: Loss = 0.10161, Accuracy = 99.85%: 100%|██████████| 572/572 [00:19<00:00, 28.68it/s]\n","[42 / 50]   Val: Loss = 43.83239, Accuracy = 94.13%: 100%|██████████| 13/13 [00:00<00:00, 35.56it/s]\n","[43 / 50] Train: Loss = 0.08709, Accuracy = 99.85%: 100%|██████████| 572/572 [00:20<00:00, 28.46it/s]\n","[43 / 50]   Val: Loss = 49.37006, Accuracy = 94.12%: 100%|██████████| 13/13 [00:00<00:00, 35.23it/s]\n","[44 / 50] Train: Loss = 0.08213, Accuracy = 99.85%: 100%|██████████| 572/572 [00:20<00:00, 28.45it/s]\n","[44 / 50]   Val: Loss = 49.88177, Accuracy = 94.09%: 100%|██████████| 13/13 [00:00<00:00, 33.59it/s]\n","[45 / 50] Train: Loss = 0.08063, Accuracy = 99.85%: 100%|██████████| 572/572 [00:20<00:00, 28.30it/s]\n","[45 / 50]   Val: Loss = 49.72467, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 32.63it/s]\n","[46 / 50] Train: Loss = 0.08058, Accuracy = 99.85%: 100%|██████████| 572/572 [00:20<00:00, 28.40it/s]\n","[46 / 50]   Val: Loss = 49.20470, Accuracy = 94.11%: 100%|██████████| 13/13 [00:00<00:00, 34.68it/s]\n","[47 / 50] Train: Loss = 0.08229, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.67it/s]\n","[47 / 50]   Val: Loss = 55.86532, Accuracy = 94.12%: 100%|██████████| 13/13 [00:00<00:00, 33.74it/s]\n","[48 / 50] Train: Loss = 0.08275, Accuracy = 99.84%: 100%|██████████| 572/572 [00:19<00:00, 28.74it/s]\n","[48 / 50]   Val: Loss = 52.98372, Accuracy = 94.12%: 100%|██████████| 13/13 [00:00<00:00, 34.85it/s]\n","[49 / 50] Train: Loss = 0.08431, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.25it/s]\n","[49 / 50]   Val: Loss = 47.31872, Accuracy = 94.09%: 100%|██████████| 13/13 [00:00<00:00, 34.55it/s]\n","[50 / 50] Train: Loss = 0.08662, Accuracy = 99.84%: 100%|██████████| 572/572 [00:20<00:00, 28.16it/s]\n","[50 / 50]   Val: Loss = 50.72600, Accuracy = 94.01%: 100%|██████████| 13/13 [00:00<00:00, 34.14it/s]\n"]}],"source":["model = LSTMTagger(\n","    vocab_size=len(word2ind),\n","    tagset_size=len(tag2ind)\n",").cuda()\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n","optimizer = optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n","    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"]},{"cell_type":"markdown","metadata":{"id":"m0qGetIhfUE5"},"source":["### Masking\n","\n","**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n","\n","У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."]},{"cell_type":"markdown","metadata":{"id":"nAfV2dEOfHo5"},"source":["**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"98wr38_rw55D","executionInfo":{"status":"ok","timestamp":1653145011519,"user_tz":-420,"elapsed":892,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"f558e6bb-008d-42c7-8ef8-6d7e8eb2e136"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy on test data = 94.08%\n"]}],"source":["def compute_accuracy(model, data):\n","\n","    model.eval() \n","\n","    with torch.no_grad():\n","\n","      correct_samples = 0 \n","      total_samples = 0\n","\n","      for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size = 64)):\n","\n","        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n","\n","        indices = torch.argmax(model(X_batch), dim = 2) \n","        mask = (y_batch != 0)\n","        correct_samples += ((indices == y_batch) * mask).sum().item()\n","        total_samples += mask.sum().item()\n","\n","      accuracy = float(correct_samples) / total_samples\n","\n","    return accuracy\n","\n","test_accuracy = compute_accuracy(model, (X_test, y_test))\n","print('Accuracy on test data = {:.2%}'.format(test_accuracy))"]},{"cell_type":"markdown","metadata":{"id":"PXUTSFaEHbDG"},"source":["### Bidirectional LSTM\n","\n","Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n","\n","![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n","*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n","\n","**Задание** Добавьте Bidirectional LSTM."]},{"cell_type":"code","execution_count":21,"metadata":{"id":"tazuWMVXruda","executionInfo":{"status":"ok","timestamp":1653145011523,"user_tz":-420,"elapsed":32,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"outputs":[],"source":["class BidirectionalLSTMTagger(nn.Module):\n","    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n","        super().__init__()\n","        \n","        self._emb = nn.Embedding(vocab_size, word_emb_dim)\n","        self._lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional = True)\n","        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n","\n","    def forward(self, inputs):\n","        emb = self._emb(inputs)\n","        output = self._lstm(emb)[0]\n","        out = self._out_layer(output)\n","        \n","        return out"]},{"cell_type":"code","source":["model = BidirectionalLSTMTagger(\n","    vocab_size=len(word2ind),\n","    tagset_size=len(tag2ind)\n",").cuda()\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n","optimizer = optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n","    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UItEkOAQ5j2j","executionInfo":{"status":"ok","timestamp":1653146133139,"user_tz":-420,"elapsed":1121645,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"89089c3f-575b-4d42-89b0-fe3744654786"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["[1 / 50] Train: Loss = 41.00727, Accuracy = 78.45%: 100%|██████████| 572/572 [00:21<00:00, 26.72it/s]\n","[1 / 50]   Val: Loss = 35.72458, Accuracy = 87.67%: 100%|██████████| 13/13 [00:00<00:00, 25.33it/s]\n","[2 / 50] Train: Loss = 17.76979, Accuracy = 90.43%: 100%|██████████| 572/572 [00:21<00:00, 26.92it/s]\n","[2 / 50]   Val: Loss = 24.05971, Accuracy = 92.01%: 100%|██████████| 13/13 [00:00<00:00, 25.46it/s]\n","[3 / 50] Train: Loss = 11.21079, Accuracy = 93.73%: 100%|██████████| 572/572 [00:21<00:00, 26.75it/s]\n","[3 / 50]   Val: Loss = 18.61242, Accuracy = 93.99%: 100%|██████████| 13/13 [00:00<00:00, 25.50it/s]\n","[4 / 50] Train: Loss = 7.51217, Accuracy = 95.49%: 100%|██████████| 572/572 [00:27<00:00, 21.14it/s]\n","[4 / 50]   Val: Loss = 14.76981, Accuracy = 94.99%: 100%|██████████| 13/13 [00:00<00:00, 25.89it/s]\n","[5 / 50] Train: Loss = 5.02467, Accuracy = 96.67%: 100%|██████████| 572/572 [00:21<00:00, 26.88it/s]\n","[5 / 50]   Val: Loss = 13.71089, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 27.22it/s]\n","[6 / 50] Train: Loss = 3.49320, Accuracy = 97.47%: 100%|██████████| 572/572 [00:21<00:00, 26.48it/s]\n","[6 / 50]   Val: Loss = 12.51292, Accuracy = 96.00%: 100%|██████████| 13/13 [00:00<00:00, 26.24it/s]\n","[7 / 50] Train: Loss = 2.49951, Accuracy = 98.08%: 100%|██████████| 572/572 [00:21<00:00, 26.43it/s]\n","[7 / 50]   Val: Loss = 12.12206, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 25.38it/s]\n","[8 / 50] Train: Loss = 1.78497, Accuracy = 98.55%: 100%|██████████| 572/572 [00:22<00:00, 25.74it/s]\n","[8 / 50]   Val: Loss = 12.32729, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 25.19it/s]\n","[9 / 50] Train: Loss = 1.29249, Accuracy = 98.90%: 100%|██████████| 572/572 [00:21<00:00, 26.71it/s]\n","[9 / 50]   Val: Loss = 12.90618, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 25.48it/s]\n","[10 / 50] Train: Loss = 0.93069, Accuracy = 99.19%: 100%|██████████| 572/572 [00:21<00:00, 26.73it/s]\n","[10 / 50]   Val: Loss = 12.75573, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 25.73it/s]\n","[11 / 50] Train: Loss = 0.66480, Accuracy = 99.40%: 100%|██████████| 572/572 [00:21<00:00, 26.85it/s]\n","[11 / 50]   Val: Loss = 13.48502, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 24.63it/s]\n","[12 / 50] Train: Loss = 0.46701, Accuracy = 99.59%: 100%|██████████| 572/572 [00:21<00:00, 26.49it/s]\n","[12 / 50]   Val: Loss = 13.26567, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 25.90it/s]\n","[13 / 50] Train: Loss = 0.33020, Accuracy = 99.71%: 100%|██████████| 572/572 [00:21<00:00, 26.42it/s]\n","[13 / 50]   Val: Loss = 14.86417, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 25.28it/s]\n","[14 / 50] Train: Loss = 0.21011, Accuracy = 99.83%: 100%|██████████| 572/572 [00:21<00:00, 26.83it/s]\n","[14 / 50]   Val: Loss = 14.21026, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 25.10it/s]\n","[15 / 50] Train: Loss = 0.12839, Accuracy = 99.91%: 100%|██████████| 572/572 [00:21<00:00, 26.05it/s]\n","[15 / 50]   Val: Loss = 15.52339, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 24.72it/s]\n","[16 / 50] Train: Loss = 0.10996, Accuracy = 99.92%: 100%|██████████| 572/572 [00:21<00:00, 26.04it/s]\n","[16 / 50]   Val: Loss = 19.69287, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 24.36it/s]\n","[17 / 50] Train: Loss = 0.37457, Accuracy = 99.73%: 100%|██████████| 572/572 [00:21<00:00, 26.33it/s]\n","[17 / 50]   Val: Loss = 17.07112, Accuracy = 96.46%: 100%|██████████| 13/13 [00:00<00:00, 25.28it/s]\n","[18 / 50] Train: Loss = 0.25555, Accuracy = 99.79%: 100%|██████████| 572/572 [00:21<00:00, 26.57it/s]\n","[18 / 50]   Val: Loss = 17.22938, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 23.86it/s]\n","[19 / 50] Train: Loss = 0.07560, Accuracy = 99.95%: 100%|██████████| 572/572 [00:21<00:00, 26.09it/s]\n","[19 / 50]   Val: Loss = 16.46168, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 23.93it/s]\n","[20 / 50] Train: Loss = 0.03575, Accuracy = 99.99%: 100%|██████████| 572/572 [00:21<00:00, 26.18it/s]\n","[20 / 50]   Val: Loss = 16.06965, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 25.31it/s]\n","[21 / 50] Train: Loss = 0.02099, Accuracy = 99.99%: 100%|██████████| 572/572 [00:21<00:00, 26.13it/s]\n","[21 / 50]   Val: Loss = 18.59679, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 25.86it/s]\n","[22 / 50] Train: Loss = 0.01455, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.38it/s]\n","[22 / 50]   Val: Loss = 18.50371, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 24.98it/s]\n","[23 / 50] Train: Loss = 0.01058, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.33it/s]\n","[23 / 50]   Val: Loss = 16.17043, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 25.16it/s]\n","[24 / 50] Train: Loss = 0.00786, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.32it/s]\n","[24 / 50]   Val: Loss = 19.65596, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 24.85it/s]\n","[25 / 50] Train: Loss = 0.00617, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.11it/s]\n","[25 / 50]   Val: Loss = 20.57583, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 24.41it/s]\n","[26 / 50] Train: Loss = 0.00452, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.98it/s]\n","[26 / 50]   Val: Loss = 19.77457, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 25.00it/s]\n","[27 / 50] Train: Loss = 0.00428, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.90it/s]\n","[27 / 50]   Val: Loss = 21.13821, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 26.05it/s]\n","[28 / 50] Train: Loss = 0.02902, Accuracy = 99.98%: 100%|██████████| 572/572 [00:21<00:00, 26.08it/s]\n","[28 / 50]   Val: Loss = 24.66059, Accuracy = 96.26%: 100%|██████████| 13/13 [00:00<00:00, 25.64it/s]\n","[29 / 50] Train: Loss = 0.87404, Accuracy = 99.39%: 100%|██████████| 572/572 [00:22<00:00, 25.97it/s]\n","[29 / 50]   Val: Loss = 19.61802, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 24.10it/s]\n","[30 / 50] Train: Loss = 0.13445, Accuracy = 99.88%: 100%|██████████| 572/572 [00:21<00:00, 26.06it/s]\n","[30 / 50]   Val: Loss = 18.67962, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 24.97it/s]\n","[31 / 50] Train: Loss = 0.02978, Accuracy = 99.99%: 100%|██████████| 572/572 [00:21<00:00, 26.12it/s]\n","[31 / 50]   Val: Loss = 19.14512, Accuracy = 96.89%: 100%|██████████| 13/13 [00:00<00:00, 25.89it/s]\n","[32 / 50] Train: Loss = 0.01541, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.20it/s]\n","[32 / 50]   Val: Loss = 18.52863, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 24.71it/s]\n","[33 / 50] Train: Loss = 0.01022, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.98it/s]\n","[33 / 50]   Val: Loss = 18.28391, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 26.53it/s]\n","[34 / 50] Train: Loss = 0.00767, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.13it/s]\n","[34 / 50]   Val: Loss = 21.97648, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 25.46it/s]\n","[35 / 50] Train: Loss = 0.00559, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.23it/s]\n","[35 / 50]   Val: Loss = 21.08166, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 24.11it/s]\n","[36 / 50] Train: Loss = 0.00440, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.28it/s]\n","[36 / 50]   Val: Loss = 21.22063, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 24.31it/s]\n","[37 / 50] Train: Loss = 0.00347, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.72it/s]\n","[37 / 50]   Val: Loss = 22.51717, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 24.09it/s]\n","[38 / 50] Train: Loss = 0.00301, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.39it/s]\n","[38 / 50]   Val: Loss = 18.43209, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 25.54it/s]\n","[39 / 50] Train: Loss = 0.00262, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.82it/s]\n","[39 / 50]   Val: Loss = 22.76380, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 25.21it/s]\n","[40 / 50] Train: Loss = 0.00263, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.82it/s]\n","[40 / 50]   Val: Loss = 21.76597, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 24.39it/s]\n","[41 / 50] Train: Loss = 0.00204, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.73it/s]\n","[41 / 50]   Val: Loss = 23.33166, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 24.55it/s]\n","[42 / 50] Train: Loss = 0.62161, Accuracy = 99.63%: 100%|██████████| 572/572 [00:21<00:00, 26.32it/s]\n","[42 / 50]   Val: Loss = 19.03831, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 26.04it/s]\n","[43 / 50] Train: Loss = 0.25748, Accuracy = 99.80%: 100%|██████████| 572/572 [00:21<00:00, 26.12it/s]\n","[43 / 50]   Val: Loss = 17.22768, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 25.49it/s]\n","[44 / 50] Train: Loss = 0.04019, Accuracy = 99.97%: 100%|██████████| 572/572 [00:21<00:00, 26.23it/s]\n","[44 / 50]   Val: Loss = 20.52827, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 25.26it/s]\n","[45 / 50] Train: Loss = 0.01279, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.76it/s]\n","[45 / 50]   Val: Loss = 18.14991, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 27.26it/s]\n","[46 / 50] Train: Loss = 0.00817, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.92it/s]\n","[46 / 50]   Val: Loss = 20.97393, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 26.37it/s]\n","[47 / 50] Train: Loss = 0.00598, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.55it/s]\n","[47 / 50]   Val: Loss = 18.33746, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 26.43it/s]\n","[48 / 50] Train: Loss = 0.00450, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.27it/s]\n","[48 / 50]   Val: Loss = 21.47557, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 24.77it/s]\n","[49 / 50] Train: Loss = 0.00355, Accuracy = 100.00%: 100%|██████████| 572/572 [00:22<00:00, 25.99it/s]\n","[49 / 50]   Val: Loss = 23.08593, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 26.44it/s]\n","[50 / 50] Train: Loss = 0.00281, Accuracy = 100.00%: 100%|██████████| 572/572 [00:21<00:00, 26.02it/s]\n","[50 / 50]   Val: Loss = 22.96953, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 25.62it/s]\n"]}]},{"cell_type":"code","source":["test_accuracy = compute_accuracy(model, (X_test, y_test))\n","print('Accuracy of BidirectionalLSTMTagger on test data = {:.2%}'.format(test_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dFur99OJ7qRS","executionInfo":{"status":"ok","timestamp":1653146134363,"user_tz":-420,"elapsed":1246,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"52dd9171-5b56-4d97-eca3-4d0b3efad855"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of BidirectionalLSTMTagger on test data = 96.93%\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZTXmYGD_ANhm"},"source":["### Предобученные эмбеддинги\n","\n","Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n","\n","Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"uZpY_Q1xZ18h","executionInfo":{"status":"ok","timestamp":1653146163653,"user_tz":-420,"elapsed":29297,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"outputs":[],"source":["import gensim.downloader as api\n","\n","w2v_model = api.load('glove-wiki-gigaword-100')"]},{"cell_type":"markdown","metadata":{"id":"KYogOoKlgtcf"},"source":["Построим подматрицу для слов из нашей тренировочной выборки:"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VsCstxiO03oT","executionInfo":{"status":"ok","timestamp":1653146163655,"user_tz":-420,"elapsed":55,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"a94748f8-ef9c-4450-a2c3-8b2753556b51"},"outputs":[{"output_type":"stream","name":"stdout","text":["Know 38736 out of 45441 word embeddings\n"]}],"source":["known_count = 0\n","embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n","for word, ind in word2ind.items():\n","    word = word.lower()\n","    if word in w2v_model.vocab:\n","        embeddings[ind] = w2v_model.get_vector(word)\n","        known_count += 1\n","        \n","print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"]},{"cell_type":"markdown","metadata":{"id":"HcG7i-R8hbY3"},"source":["**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."]},{"cell_type":"code","source":["from torch.autograd import Variable\n","\n","class LSTMTaggerWithPretrainedEmbs(nn.Module):\n","    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n","        super().__init__()\n","        \n","\n","        embeddings_t = Variable(torch.from_numpy(embeddings)).float().cuda()\n","\n","        self._emb = nn.Embedding.from_pretrained(embeddings_t)\n","        self._lstm = nn.LSTM(embeddings_t.shape[1], \n","                             lstm_hidden_dim, \n","                             num_layers=lstm_layers_count, \n","                             bidirectional = True)\n","        self._out_layer = nn.Linear(2*lstm_hidden_dim, tagset_size)\n","\n","    def forward(self, inputs):\n","        emb = self._emb(inputs)\n","        output = self._lstm(emb)[0]\n","        out = self._out_layer(output)\n","        return out"],"metadata":{"id":"DfQnoJIo-L-p","executionInfo":{"status":"ok","timestamp":1653146163656,"user_tz":-420,"elapsed":42,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EBtI6BDE-Fc7","executionInfo":{"status":"ok","timestamp":1653147006895,"user_tz":-420,"elapsed":843280,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"ff09a1e3-58b2-42c4-d9bb-2ef884d48c92"},"outputs":[{"output_type":"stream","name":"stderr","text":["[1 / 50] Train: Loss = 43.68784, Accuracy = 79.54%: 100%|██████████| 572/572 [00:16<00:00, 33.87it/s]\n","[1 / 50]   Val: Loss = 36.43289, Accuracy = 89.45%: 100%|██████████| 13/13 [00:00<00:00, 35.87it/s]\n","[2 / 50] Train: Loss = 16.75419, Accuracy = 91.80%: 100%|██████████| 572/572 [00:16<00:00, 34.79it/s]\n","[2 / 50]   Val: Loss = 23.19057, Accuracy = 92.44%: 100%|██████████| 13/13 [00:00<00:00, 38.58it/s]\n","[3 / 50] Train: Loss = 12.55415, Accuracy = 93.70%: 100%|██████████| 572/572 [00:16<00:00, 34.86it/s]\n","[3 / 50]   Val: Loss = 19.92942, Accuracy = 93.55%: 100%|██████████| 13/13 [00:00<00:00, 38.06it/s]\n","[4 / 50] Train: Loss = 10.20120, Accuracy = 94.72%: 100%|██████████| 572/572 [00:16<00:00, 35.62it/s]\n","[4 / 50]   Val: Loss = 16.83255, Accuracy = 94.39%: 100%|██████████| 13/13 [00:00<00:00, 40.02it/s]\n","[5 / 50] Train: Loss = 8.52363, Accuracy = 95.35%: 100%|██████████| 572/572 [00:16<00:00, 35.29it/s]\n","[5 / 50]   Val: Loss = 15.72614, Accuracy = 94.81%: 100%|██████████| 13/13 [00:00<00:00, 37.39it/s]\n","[6 / 50] Train: Loss = 7.41121, Accuracy = 95.86%: 100%|██████████| 572/572 [00:16<00:00, 35.17it/s]\n","[6 / 50]   Val: Loss = 13.41272, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 41.09it/s]\n","[7 / 50] Train: Loss = 6.39262, Accuracy = 96.20%: 100%|██████████| 572/572 [00:16<00:00, 35.16it/s]\n","[7 / 50]   Val: Loss = 12.78322, Accuracy = 95.59%: 100%|██████████| 13/13 [00:00<00:00, 35.99it/s]\n","[8 / 50] Train: Loss = 5.75709, Accuracy = 96.50%: 100%|██████████| 572/572 [00:17<00:00, 33.50it/s]\n","[8 / 50]   Val: Loss = 12.46195, Accuracy = 95.70%: 100%|██████████| 13/13 [00:00<00:00, 37.78it/s]\n","[9 / 50] Train: Loss = 5.13313, Accuracy = 96.73%: 100%|██████████| 572/572 [00:17<00:00, 33.42it/s]\n","[9 / 50]   Val: Loss = 11.40001, Accuracy = 95.95%: 100%|██████████| 13/13 [00:00<00:00, 36.88it/s]\n","[10 / 50] Train: Loss = 4.66014, Accuracy = 96.93%: 100%|██████████| 572/572 [00:16<00:00, 34.66it/s]\n","[10 / 50]   Val: Loss = 10.55550, Accuracy = 96.12%: 100%|██████████| 13/13 [00:00<00:00, 37.90it/s]\n","[11 / 50] Train: Loss = 4.31218, Accuracy = 97.10%: 100%|██████████| 572/572 [00:16<00:00, 34.14it/s]\n","[11 / 50]   Val: Loss = 10.81554, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 35.92it/s]\n","[12 / 50] Train: Loss = 4.02270, Accuracy = 97.24%: 100%|██████████| 572/572 [00:16<00:00, 34.76it/s]\n","[12 / 50]   Val: Loss = 11.20943, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 36.45it/s]\n","[13 / 50] Train: Loss = 3.68542, Accuracy = 97.39%: 100%|██████████| 572/572 [00:16<00:00, 35.47it/s]\n","[13 / 50]   Val: Loss = 10.22405, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 37.45it/s]\n","[14 / 50] Train: Loss = 3.45339, Accuracy = 97.50%: 100%|██████████| 572/572 [00:16<00:00, 34.35it/s]\n","[14 / 50]   Val: Loss = 10.54092, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 37.93it/s]\n","[15 / 50] Train: Loss = 3.22484, Accuracy = 97.60%: 100%|██████████| 572/572 [00:16<00:00, 34.91it/s]\n","[15 / 50]   Val: Loss = 8.80767, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 40.28it/s]\n","[16 / 50] Train: Loss = 3.02159, Accuracy = 97.73%: 100%|██████████| 572/572 [00:15<00:00, 36.07it/s]\n","[16 / 50]   Val: Loss = 10.03057, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 39.67it/s]\n","[17 / 50] Train: Loss = 2.84377, Accuracy = 97.81%: 100%|██████████| 572/572 [00:16<00:00, 34.24it/s]\n","[17 / 50]   Val: Loss = 9.24700, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 36.55it/s]\n","[18 / 50] Train: Loss = 2.67419, Accuracy = 97.90%: 100%|██████████| 572/572 [00:17<00:00, 32.57it/s]\n","[18 / 50]   Val: Loss = 9.78303, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 36.07it/s]\n","[19 / 50] Train: Loss = 2.52376, Accuracy = 97.98%: 100%|██████████| 572/572 [00:17<00:00, 33.16it/s]\n","[19 / 50]   Val: Loss = 9.78103, Accuracy = 96.60%: 100%|██████████| 13/13 [00:00<00:00, 37.60it/s]\n","[20 / 50] Train: Loss = 2.43169, Accuracy = 98.03%: 100%|██████████| 572/572 [00:16<00:00, 34.28it/s]\n","[20 / 50]   Val: Loss = 9.67905, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 35.72it/s]\n","[21 / 50] Train: Loss = 2.28590, Accuracy = 98.11%: 100%|██████████| 572/572 [00:16<00:00, 35.10it/s]\n","[21 / 50]   Val: Loss = 10.73003, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 36.91it/s]\n","[22 / 50] Train: Loss = 2.17272, Accuracy = 98.17%: 100%|██████████| 572/572 [00:16<00:00, 33.83it/s]\n","[22 / 50]   Val: Loss = 9.69489, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 39.98it/s]\n","[23 / 50] Train: Loss = 2.09551, Accuracy = 98.23%: 100%|██████████| 572/572 [00:16<00:00, 34.14it/s]\n","[23 / 50]   Val: Loss = 9.59617, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 35.22it/s]\n","[24 / 50] Train: Loss = 1.97545, Accuracy = 98.31%: 100%|██████████| 572/572 [00:16<00:00, 35.04it/s]\n","[24 / 50]   Val: Loss = 9.99802, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 37.19it/s]\n","[25 / 50] Train: Loss = 1.87946, Accuracy = 98.36%: 100%|██████████| 572/572 [00:16<00:00, 35.14it/s]\n","[25 / 50]   Val: Loss = 9.70555, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 40.88it/s]\n","[26 / 50] Train: Loss = 1.78816, Accuracy = 98.42%: 100%|██████████| 572/572 [00:16<00:00, 34.64it/s]\n","[26 / 50]   Val: Loss = 10.08344, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 38.62it/s]\n","[27 / 50] Train: Loss = 1.68923, Accuracy = 98.48%: 100%|██████████| 572/572 [00:16<00:00, 34.48it/s]\n","[27 / 50]   Val: Loss = 11.03537, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 37.65it/s]\n","[28 / 50] Train: Loss = 1.64618, Accuracy = 98.52%: 100%|██████████| 572/572 [00:16<00:00, 35.23it/s]\n","[28 / 50]   Val: Loss = 10.26036, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 39.12it/s]\n","[29 / 50] Train: Loss = 1.57741, Accuracy = 98.58%: 100%|██████████| 572/572 [00:16<00:00, 34.72it/s]\n","[29 / 50]   Val: Loss = 9.87138, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 36.93it/s]\n","[30 / 50] Train: Loss = 1.48302, Accuracy = 98.64%: 100%|██████████| 572/572 [00:16<00:00, 34.33it/s]\n","[30 / 50]   Val: Loss = 11.92901, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 36.44it/s]\n","[31 / 50] Train: Loss = 1.42588, Accuracy = 98.68%: 100%|██████████| 572/572 [00:16<00:00, 34.45it/s]\n","[31 / 50]   Val: Loss = 10.17109, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 39.66it/s]\n","[32 / 50] Train: Loss = 1.36276, Accuracy = 98.73%: 100%|██████████| 572/572 [00:16<00:00, 35.28it/s]\n","[32 / 50]   Val: Loss = 11.45088, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 36.78it/s]\n","[33 / 50] Train: Loss = 1.30672, Accuracy = 98.77%: 100%|██████████| 572/572 [00:16<00:00, 35.65it/s]\n","[33 / 50]   Val: Loss = 10.46445, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 38.52it/s]\n","[34 / 50] Train: Loss = 1.25769, Accuracy = 98.82%: 100%|██████████| 572/572 [00:16<00:00, 35.55it/s]\n","[34 / 50]   Val: Loss = 13.98356, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 37.35it/s]\n","[35 / 50] Train: Loss = 1.20851, Accuracy = 98.86%: 100%|██████████| 572/572 [00:16<00:00, 35.74it/s]\n","[35 / 50]   Val: Loss = 10.78368, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 37.84it/s]\n","[36 / 50] Train: Loss = 1.13889, Accuracy = 98.91%: 100%|██████████| 572/572 [00:15<00:00, 35.79it/s]\n","[36 / 50]   Val: Loss = 12.77421, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 36.95it/s]\n","[37 / 50] Train: Loss = 1.08093, Accuracy = 98.95%: 100%|██████████| 572/572 [00:15<00:00, 36.34it/s]\n","[37 / 50]   Val: Loss = 12.18805, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 41.02it/s]\n","[38 / 50] Train: Loss = 1.04577, Accuracy = 98.99%: 100%|██████████| 572/572 [00:15<00:00, 35.77it/s]\n","[38 / 50]   Val: Loss = 11.89125, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 38.87it/s]\n","[39 / 50] Train: Loss = 0.99550, Accuracy = 99.03%: 100%|██████████| 572/572 [00:15<00:00, 35.79it/s]\n","[39 / 50]   Val: Loss = 11.78159, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 38.48it/s]\n","[40 / 50] Train: Loss = 0.97619, Accuracy = 99.06%: 100%|██████████| 572/572 [00:16<00:00, 35.29it/s]\n","[40 / 50]   Val: Loss = 14.52576, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 38.46it/s]\n","[41 / 50] Train: Loss = 0.90518, Accuracy = 99.13%: 100%|██████████| 572/572 [00:16<00:00, 34.14it/s]\n","[41 / 50]   Val: Loss = 12.46983, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 36.28it/s]\n","[42 / 50] Train: Loss = 0.86538, Accuracy = 99.15%: 100%|██████████| 572/572 [00:16<00:00, 35.38it/s]\n","[42 / 50]   Val: Loss = 13.87710, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 38.87it/s]\n","[43 / 50] Train: Loss = 0.83233, Accuracy = 99.19%: 100%|██████████| 572/572 [00:15<00:00, 35.78it/s]\n","[43 / 50]   Val: Loss = 13.58754, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 40.50it/s]\n","[44 / 50] Train: Loss = 0.80573, Accuracy = 99.22%: 100%|██████████| 572/572 [00:16<00:00, 35.51it/s]\n","[44 / 50]   Val: Loss = 15.13611, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 35.68it/s]\n","[45 / 50] Train: Loss = 0.75432, Accuracy = 99.26%: 100%|██████████| 572/572 [00:15<00:00, 35.78it/s]\n","[45 / 50]   Val: Loss = 14.27783, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 39.45it/s]\n","[46 / 50] Train: Loss = 0.73752, Accuracy = 99.27%: 100%|██████████| 572/572 [00:15<00:00, 36.23it/s]\n","[46 / 50]   Val: Loss = 14.48606, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 39.32it/s]\n","[47 / 50] Train: Loss = 0.68432, Accuracy = 99.33%: 100%|██████████| 572/572 [00:16<00:00, 35.19it/s]\n","[47 / 50]   Val: Loss = 14.15750, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 37.38it/s]\n","[48 / 50] Train: Loss = 0.66396, Accuracy = 99.35%: 100%|██████████| 572/572 [00:15<00:00, 36.30it/s]\n","[48 / 50]   Val: Loss = 16.07199, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 39.58it/s]\n","[49 / 50] Train: Loss = 0.63639, Accuracy = 99.38%: 100%|██████████| 572/572 [00:16<00:00, 35.72it/s]\n","[49 / 50]   Val: Loss = 16.03653, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 38.07it/s]\n","[50 / 50] Train: Loss = 0.60029, Accuracy = 99.41%: 100%|██████████| 572/572 [00:16<00:00, 35.42it/s]\n","[50 / 50]   Val: Loss = 16.21541, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 37.46it/s]\n"]}],"source":["model = LSTMTaggerWithPretrainedEmbs(\n","    embeddings=embeddings,\n","    tagset_size=len(tag2ind)\n",").cuda()\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n","optimizer = optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n","    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"]},{"cell_type":"markdown","metadata":{"id":"2Ne_8f24h8kg"},"source":["**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n","\n","Добейтесь качества лучше прошлых моделей."]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HPUuAPGhEGVR","executionInfo":{"status":"ok","timestamp":1653147060012,"user_tz":-420,"elapsed":58364,"user":{"displayName":"Вадим К","userId":"15055313511788181293"}},"outputId":"c841cbb3-e909-4a04-abb3-849d91176022"},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of LSTMTaggerWithPretrainedEmbs on test data = 96.72%\n"]}],"source":["test_accuracy = compute_accuracy(model, (X_test, y_test))\n","print('Accuracy of LSTMTaggerWithPretrainedEmbs on test data = {:.2%}'.format(test_accuracy))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"RNNs.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}